<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>进制转换</title>
      <link href="2020/02/20/%E5%A0%86%E6%A0%88%E8%BF%9B%E8%A1%8C%E8%BF%9B%E5%88%B6%E8%BD%AC%E6%8D%A2/"/>
      <url>2020/02/20/%E5%A0%86%E6%A0%88%E8%BF%9B%E8%A1%8C%E8%BF%9B%E5%88%B6%E8%BD%AC%E6%8D%A2/</url>
      
        <content type="html"><![CDATA[<br><h4 id="这是一种比较简单的方式进行进制转换，利用堆栈的先进后出，先求余后放入栈中，再从栈中拿出来。"><a href="#这是一种比较简单的方式进行进制转换，利用堆栈的先进后出，先求余后放入栈中，再从栈中拿出来。" class="headerlink" title="这是一种比较简单的方式进行进制转换，利用堆栈的先进后出，先求余后放入栈中，再从栈中拿出来。"></a>这是一种比较简单的方式进行进制转换，利用堆栈的先进后出，先求余后放入栈中，再从栈中拿出来。</h4><p><img src="https://img-blog.csdnimg.cn/20200226104451448.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzkxMTM5,size_16,color_FFFFFF,t_70"></p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;iostream></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;stack></span></span>using namespace std<span class="token punctuation">;</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">&#123;</span>    stack <span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span>v<span class="token punctuation">;</span>    <span class="token keyword">int</span> n<span class="token punctuation">,</span>d<span class="token punctuation">;</span>    cout <span class="token operator">&lt;&lt;</span> <span class="token string">"请输入你要转换的十进制数:"</span><span class="token punctuation">;</span>    cin <span class="token operator">>></span> n<span class="token punctuation">;</span>    cout <span class="token operator">&lt;&lt;</span> <span class="token string">"请输入你的目标进制数:"</span><span class="token punctuation">;</span>    cin <span class="token operator">>></span> d<span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span>    <span class="token punctuation">&#123;</span>        v<span class="token punctuation">.</span><span class="token function">push</span><span class="token punctuation">(</span>n <span class="token operator">%</span> d<span class="token punctuation">)</span><span class="token punctuation">;</span>        n <span class="token operator">=</span> n <span class="token operator">/</span> d<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span><span class="token operator">!</span>v<span class="token punctuation">.</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">&#123;</span>        cout <span class="token operator">&lt;&lt;</span> v<span class="token punctuation">.</span><span class="token function">top</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        v<span class="token punctuation">.</span><span class="token function">pop</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment">///pop的返回值类型为空</span>    <span class="token punctuation">&#125;</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 进制 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>排序算法</title>
      <link href="2020/02/20/%E6%8E%92%E5%BA%8F%E5%88%86%E7%B1%BB/"/>
      <url>2020/02/20/%E6%8E%92%E5%BA%8F%E5%88%86%E7%B1%BB/</url>
      
        <content type="html"><![CDATA[<h2 id="1-排序分类"><a href="#1-排序分类" class="headerlink" title="1.排序分类"></a>1.排序分类</h2><p>十种常见排序算法可以分为两大类：</p><ul><li><strong>比较类排序</strong>：通过比较来决定元素间的相对次序，由于其时间复杂度不能突破O(nlogn)，因此也称为非线性时间比较类排序。</li><li><strong>非比较类排序</strong>：不通过比较来决定元素间的相对次序，它可以突破基于比较排序的时间下界，以线性时间运行，因此也称为线性时间非比较类排序。 </li><li><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1592446740975-a94fca34-801d-432b-adb9-ab7d1587a3c1.png" alt="image"></li></ul><h2 id="2-排序时间复杂度分析"><a href="#2-排序时间复杂度分析" class="headerlink" title="2.排序时间复杂度分析"></a>2.排序时间复杂度分析</h2><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1592446752914-e0ac991e-b671-4fc4-8568-43e8457341cd.png" alt="image"></p><h2 id="3-排序算法总结"><a href="#3-排序算法总结" class="headerlink" title="3.排序算法总结"></a>3.排序算法总结</h2><h3 id="1-冒泡排序-（交换类"><a href="#1-冒泡排序-（交换类" class="headerlink" title="1.冒泡排序:（交换类)"></a>1.冒泡排序:（交换类)</h3><p>   冒泡排序是一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。 </p><h4 id="1-1-算法描述"><a href="#1-1-算法描述" class="headerlink" title="1.1 算法描述"></a>1.1 算法描述</h4><ul><li>比较相邻的元素。如果第一个比第二个大，就交换它们两个；</li><li>对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对，这样在最后的元素应该会是最大的数；</li><li>针对所有的元素重复以上的步骤，除了最后一个；</li><li>重复步骤1~3，直到排序完成。</li></ul><h4 id="1-2-动图演示"><a href="#1-2-动图演示" class="headerlink" title="1.2 动图演示"></a>1.2 动图演示</h4><p><img src="https://cdn.nlark.com/yuque/0/2020/gif/1489802/1592446841226-24efd00e-49e4-4215-8d26-05cc5a84cf29.gif" alt="image"></p><h4 id="1-3-代码演示"><a href="#1-3-代码演示" class="headerlink" title="1.3 代码演示"></a>1.3 代码演示</h4><pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token keyword">void</span> <span class="token class-name">BubbleSort</span><span class="token punctuation">(</span><span class="token keyword">int</span> a<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">&#123;</span>    <span class="token keyword">int</span> len<span class="token operator">=</span>a<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>len<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">&#123;</span>        <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> j<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>j<span class="token operator">&lt;</span>len<span class="token operator">-</span><span class="token number">1</span><span class="token operator">-</span>i<span class="token punctuation">;</span>j<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">&#123;</span>           <span class="token keyword">if</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token operator">></span>a<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">&#123;</span>  <span class="token comment">//相邻的元素两两相互比较</span>            <span class="token keyword">int</span> temp<span class="token operator">=</span>a<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token comment">//元素交换</span>             a<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">=</span>a<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>             a<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token operator">=</span>temp<span class="token punctuation">;</span>            <span class="token punctuation">&#125;</span>        <span class="token punctuation">&#125;</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="1-4-排序分析"><a href="#1-4-排序分析" class="headerlink" title="1.4 排序分析"></a>1.4 排序分析</h4><p>算法时间最坏情况：O(n²)</p><p>算法时间最好情况:  O(n)</p><p>算法时间平均情况:  O(n²) </p><p>算法空间复杂度：  O(1)</p><p>算法稳定性：      稳定</p><h3 id="2-快速排序-交换类"><a href="#2-快速排序-交换类" class="headerlink" title="2.快速排序:(交换类)"></a>2.快速排序:(交换类)</h3><p>​     快速排序的基本思想：通过一趟排序将待排记录分隔成独立的两部分，其中一部分记录的关键字均比另一部分的关键字小，则可分别对这两部分记录继续进行排序，以达到整个序列有序。</p><h4 id="2-1-算法描述"><a href="#2-1-算法描述" class="headerlink" title="2.1 算法描述"></a>2.1 算法描述</h4><p>快速排序使用分治法来把一个串（list）分为两个子串（sub-lists）。具体算法描述如下：</p><ul><li>从数列中挑出一个元素，称为 “基准”（pivot）；</li><li>重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作；</li><li>递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序</li></ul><h4 id="2-2-动图演示"><a href="#2-2-动图演示" class="headerlink" title="2.2 动图演示"></a>2.2 动图演示</h4><p><img src="https://cdn.nlark.com/yuque/0/2020/gif/1489802/1592448061916-d754eceb-2b48-4428-8407-f1d357e751c3.gif" alt="image"></p><h4 id="2-3-代码演示"><a href="#2-3-代码演示" class="headerlink" title="2.3 代码演示"></a>2.3 代码演示</h4><pre class="line-numbers language-none"><code class="language-none">void quickSort( int arr[] ,int left ,int right)&#123;        int len&#x3D;arr.size();          &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-4-排序分析"><a href="#2-4-排序分析" class="headerlink" title="2.4 排序分析"></a>2.4 排序分析</h4><p>算法时间最坏情况：O(n2)</p><p>算法时间最好情况:  O(nlog2n)</p><p>算法时间平均情况:  O(nlog2n) </p><p>算法空间复杂度：  O(nlog2n)</p><p>算法稳定性：      不稳定</p><h3 id="3-插入排序-（插入类"><a href="#3-插入排序-（插入类" class="headerlink" title="3.插入排序:（插入类)"></a>3.插入排序:（插入类)</h3><p>​     插入排序（Insertion-Sort）的算法描述是一种简单直观的排序算法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。</p><h4 id="3-1-算法描述"><a href="#3-1-算法描述" class="headerlink" title="3.1 算法描述"></a>3.1 算法描述</h4><p>一般来说，插入排序都采用in-place在数组上实现。具体算法描述如下：</p><ul><li>从第一个元素开始，该元素可以认为已经被排序；</li><li>取出下一个元素，在已经排序的元素序列中从后向前扫描；</li><li>如果该元素（已排序）大于新元素，将该元素移到下一位置；</li><li>重复步骤3，直到找到已排序的元素小于或者等于新元素的位置；</li><li>将新元素插入到该位置后；</li><li>重复步骤2~5。</li></ul><h4 id="3-2-动图演示"><a href="#3-2-动图演示" class="headerlink" title="3.2 动图演示"></a>3.2 动图演示</h4><p><img src="https://cdn.nlark.com/yuque/0/2020/gif/1489802/1592448913648-239be5e4-8f1f-49ec-a1fa-b193e461f737.gif" alt="image"></p><h4 id="3-3-代码演示"><a href="#3-3-代码演示" class="headerlink" title="3.3 代码演示"></a>3.3 代码演示</h4><pre class="line-numbers language-none"><code class="language-none">void insertionSort(int a[])&#123;    int len&#x3D;a.size();    int preIndex,current;    for(int i&#x3D;1;i&lt;len;i++)&#123;       preIndex&#x3D;i-1;       current&#x3D;a[i];       while(preIndex&gt;&#x3D;0 &amp;&amp; a[preIndex] &gt; current)&#123;           a[preIndex+1]&#x3D;a[preIndex];           preIndex--;       &#125;        a[preIndex+1] &#x3D; current;    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="3-4-排序分析"><a href="#3-4-排序分析" class="headerlink" title="3.4 排序分析"></a>3.4 排序分析</h4><p>算法时间最坏情况：O(n2)</p><p>算法时间最好情况:  O(n)</p><p>算法时间平均情况:  O(n2) </p><p>算法空间复杂度：  O(1)</p><p>算法稳定性：      稳定</p><h3 id="4-希尔排序-（插入类"><a href="#4-希尔排序-（插入类" class="headerlink" title="4.希尔排序:（插入类)"></a>4.希尔排序:（插入类)</h3><p>1959年Shell发明，第一个突破O(n2)的排序算法，是简单插入排序的改进版。它与插入排序的不同之处在于，它会优先比较距离较远的元素。希尔排序又叫<strong>缩小增量排序</strong>。</p><h4 id="4-1-算法描述"><a href="#4-1-算法描述" class="headerlink" title="4.1 算法描述"></a>4.1 算法描述</h4><p>先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，具体算法描述：</p><ul><li>选择一个增量序列t1，t2，…，tk，其中ti&gt;tj，tk=1；</li><li>按增量序列个数k，对序列进行k 趟排序；</li><li>每趟排序，根据对应的增量ti，将待排序列分割成若干长度为m 的子序列，分别对各子表进行直接插入排序。仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。</li></ul><h4 id="4-2-动图演示"><a href="#4-2-动图演示" class="headerlink" title="4.2 动图演示"></a>4.2 动图演示</h4><p><img src="https://cdn.nlark.com/yuque/0/2020/gif/1489802/1592449916429-cd0092cf-4870-4a80-aa8c-a8c3a7fee727.gif" alt="image"></p><h4 id="4-3-代码实现"><a href="#4-3-代码实现" class="headerlink" title="4.3 代码实现"></a>4.3 代码实现</h4><h4 id="4-4-排序分析"><a href="#4-4-排序分析" class="headerlink" title="4.4 排序分析"></a>4.4 排序分析</h4><p>算法时间最坏情况：O(n2)</p><p>算法时间最好情况:  O(n)</p><p>算法时间平均情况:  O(n1.3) </p><p>算法空间复杂度：  O(1)</p><p>算法稳定性：      不稳定</p><h3 id="5-选择排序-（选择类"><a href="#5-选择排序-（选择类" class="headerlink" title="5.选择排序:（选择类)"></a>5.选择排序:（选择类)</h3><p>选择排序(Selection-sort)是一种简单直观的排序算法。它的工作原理：首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。 </p><h4 id="5-1-算法描述"><a href="#5-1-算法描述" class="headerlink" title="5.1 算法描述"></a>5.1 算法描述</h4><p>n个记录的直接选择排序可经过n-1趟直接选择排序得到有序结果。具体算法描述如下：</p><ul><li>初始状态：无序区为R[1..n]，有序区为空；</li><li>第i趟排序(i=1,2,3…n-1)开始时，当前有序区和无序区分别为R[1..i-1]和R(i..n）。该趟排序从当前无序区中-选出关键字最小的记录 R[k]，将它与无序区的第1个记录R交换，使R[1..i]和R[i+1..n)分别变为记录个数增加1个的新有序区和记录个数减少1个的新无序区；</li><li>n-1趟结束，数组有序化了。</li></ul><h4 id="5-2-动图演示"><a href="#5-2-动图演示" class="headerlink" title="5.2 动图演示"></a><strong>5.2 动图演示</strong></h4><p>**<br>**<img src="https://cdn.nlark.com/yuque/0/2020/gif/1489802/1592450252179-148507bd-0fe1-4f12-8849-f0d60e864222.gif" alt="image"></p><h4 id="5-3-代码实现"><a href="#5-3-代码实现" class="headerlink" title="5.3 代码实现"></a>5.3 代码实现</h4><pre class="line-numbers language-none"><code class="language-none">void selectSort(int a[])&#123;   int len&#x3D;a.size();   int minIndex,temp;   for(int i&#x3D;0;i&lt;len-1;i++)&#123;       minIndex&#x3D;i;      for(int j&#x3D;i+1;j&lt;len;j++)&#123;        if(a[j]&lt;a[minIndex])&#123;  &#x2F;&#x2F;找最小的元素           minIndex&#x3D;j;        &#x2F;&#x2F;保存最小元素索引，以便以后交换        &#125;      &#125;       temp&#x3D;a[i];       a[i]&#x3D;a[minIndex];       a[minIndex]&#x3D;temp;   &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="5-4-排序分析"><a href="#5-4-排序分析" class="headerlink" title="5.4 排序分析"></a>5.4 排序分析</h4><p>算法时间最坏情况：O(n2)</p><p>算法时间最好情况:  O(n2)</p><p>算法时间平均情况:  O(n2) </p><p>算法空间复杂度：  O(1)</p><p>算法稳定性：      不稳定</p><h3 id="6-堆排序-（选择类"><a href="#6-堆排序-（选择类" class="headerlink" title="6.堆排序:（选择类)"></a>6.堆排序:（选择类)</h3><p>​      堆排序（Heapsort）是指利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于（或者大于）它的父节点。</p><h4 id="6-1-算法描述"><a href="#6-1-算法描述" class="headerlink" title="6.1 算法描述"></a>6.1 算法描述</h4><ul><li>将初始待排序关键字序列(R1,R2….Rn)构建成大顶堆，此堆为初始的无序区；</li><li>将堆顶元素R[1]与最后一个元素R[n]交换，此时得到新的无序区(R1,R2,……Rn-1)和新的有序区(Rn),且满足R[1,2…n-1]&lt;=R[n]；</li><li>由于交换后新的堆顶R[1]可能违反堆的性质，因此需要对当前无序区(R1,R2,……Rn-1)调整为新堆，然后再次将R[1]与无序区最后一个元素交换，得到新的无序区(R1,R2….Rn-2)和新的有序区(Rn-1,Rn)。不断重复此过程直到有序区的元素个数为n-1，则整个排序过程完成。</li></ul><h4 id="6-2-动图演示"><a href="#6-2-动图演示" class="headerlink" title="6.2 动图演示"></a>6.2 动图演示</h4><p><img src="https://cdn.nlark.com/yuque/0/2020/gif/1489802/1592451125172-4bf98683-cb34-4f8a-ae90-9e4597cdd0f6.gif" alt="image"></p><h4 id="6-3-代码实现"><a href="#6-3-代码实现" class="headerlink" title="6.3 代码实现"></a>6.3 代码实现</h4><h4 id="6-4-排序分析"><a href="#6-4-排序分析" class="headerlink" title="6.4 排序分析"></a>6.4 排序分析</h4><p>算法时间最坏情况：O(nlog2n)</p><p>算法时间最好情况:  O(nlog2n)</p><p>算法时间平均情况:  O(nlog2n)</p><p>算法空间复杂度：  O(1)</p><p>算法稳定性：      不稳定</p><h3 id="7-归并排序"><a href="#7-归并排序" class="headerlink" title="7.归并排序:"></a>7.归并排序:</h3><p>​       归并排序是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为2-路归并。 </p><h4 id="7-1-算法描述"><a href="#7-1-算法描述" class="headerlink" title="7.1 算法描述"></a>7.1 算法描述</h4><ul><li>把长度为n的输入序列分成两个长度为n/2的子序列；</li><li>对这两个子序列分别采用归并排序；</li><li>将两个排序好的子序列合并成一个最终的排序序列。</li></ul><h4 id="7-2-动图演示"><a href="#7-2-动图演示" class="headerlink" title="7.2 动图演示"></a>7.2 动图演示</h4><p><img src="https://cdn.nlark.com/yuque/0/2020/gif/1489802/1592451360105-19aeab7d-8a7a-43b6-b9b6-4e1157c3069c.gif" alt="image"></p><h4 id="7-3-代码实现"><a href="#7-3-代码实现" class="headerlink" title="7.3 代码实现"></a>7.3 代码实现</h4><h4 id="7-4-算法分析"><a href="#7-4-算法分析" class="headerlink" title="7.4 算法分析"></a>7.4 算法分析</h4><p>算法时间最坏情况：O(nlog2n)</p><p>算法时间最好情况:  O(nlog2n)</p><p>算法时间平均情况:  O(nlog2n)</p><p>算法空间复杂度：  O(n)</p><p>算法稳定性：      稳定</p><h3 id="8-计数排序"><a href="#8-计数排序" class="headerlink" title="8.计数排序:"></a>8.计数排序:</h3><p>计数排序不是基于比较的排序算法，其核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数</p><h4 id="8-1-算法描述"><a href="#8-1-算法描述" class="headerlink" title="8.1 算法描述"></a>8.1 算法描述</h4><ul><li>找出待排序的数组中最大和最小的元素；</li><li>统计数组中每个值为i的元素出现的次数，存入数组C的第i项；</li><li>对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加）；</li><li>反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1。</li></ul><h4 id="8-2-动图演示"><a href="#8-2-动图演示" class="headerlink" title="8.2 动图演示"></a>8.2 动图演示</h4><p><img src="https://cdn.nlark.com/yuque/0/2020/gif/1489802/1592451501437-220f396e-c90f-4c6d-b56e-d8cd5edbc5a9.gif" alt="image"></p><h4 id="8-3-代码实现"><a href="#8-3-代码实现" class="headerlink" title="8.3 代码实现"></a>8.3 代码实现</h4><h4 id="8-4-算法分析"><a href="#8-4-算法分析" class="headerlink" title="8.4 算法分析"></a>8.4 算法分析</h4><p>算法时间最坏情况：O(n+k)</p><p>算法时间最好情况:  O(n+k)</p><p>算法时间平均情况:  O(n+k)</p><p>算法空间复杂度：  O(n+k)</p><p>算法稳定性：      稳定</p><h3 id="9-桶排序"><a href="#9-桶排序" class="headerlink" title="9.桶排序:"></a>9.桶排序:</h3><p>​      桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。桶排序 (Bucket sort)的工作的原理：假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排）。</p><h4 id="9-1-算法描述"><a href="#9-1-算法描述" class="headerlink" title="9.1 算法描述"></a>9.1 算法描述</h4><ul><li>设置一个定量的数组当作空桶；</li><li>遍历输入数据，并且把数据一个一个放到对应的桶里去；</li><li>对每个不是空的桶进行排序；</li><li>从不是空的桶里把排好序的数据拼接起来。 </li></ul><h4 id="9-2-图片演示"><a href="#9-2-图片演示" class="headerlink" title="9.2 图片演示"></a>9.2 图片演示</h4><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1592451649373-f9567501-f8fe-4fa1-a339-913acc8a926c.png" alt="image"></p><h4 id="9-3-代码实现"><a href="#9-3-代码实现" class="headerlink" title="9.3 代码实现"></a>9.3 代码实现</h4><h4 id="9-4-算法分析"><a href="#9-4-算法分析" class="headerlink" title="9.4 算法分析"></a>9.4 算法分析</h4><p>算法时间最坏情况：O(n2)</p><p>算法时间最好情况:  O(n)</p><p>算法时间平均情况:  O(n+k)</p><p>算法空间复杂度：  O(n+k)</p><p>算法稳定性：      稳定</p><h3 id="10-基数排序"><a href="#10-基数排序" class="headerlink" title="10.基数排序:"></a>10.基数排序:</h3><p> 基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序。最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前。</p><h4 id="10-1-算法描述"><a href="#10-1-算法描述" class="headerlink" title="10.1 算法描述"></a>10.1 算法描述</h4><ul><li>取得数组中的最大数，并取得位数；</li><li>arr为原始数组，从最低位开始取每个位组成radix数组；</li><li>对radix进行计数排序（利用计数排序适用于小范围数的特点）；</li></ul><h4 id="10-2-动图演示"><a href="#10-2-动图演示" class="headerlink" title="10.2 动图演示"></a>10.2 动图演示</h4><p><img src="https://cdn.nlark.com/yuque/0/2020/gif/1489802/1592451822750-8cde53e9-1bf4-41f0-8d0a-abec9cc4186e.gif" alt="image"></p><h4 id="10-3-代码实现"><a href="#10-3-代码实现" class="headerlink" title="10.3 代码实现"></a>10.3 代码实现</h4><h4 id="10-4-算法分析"><a href="#10-4-算法分析" class="headerlink" title="10.4 算法分析"></a>10.4 算法分析</h4><p>算法时间最坏情况：O(n*k)</p><p>算法时间最好情况:  O(n*k)</p><p>算法时间平均情况:  O(n*k)</p><p>算法空间复杂度：  O(n+k)</p><p>算法稳定性：      稳定</p>]]></content>
      
      
      <categories>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 排序算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS</title>
      <link href="2020/02/20/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8BHadoop(HDFS)/"/>
      <url>2020/02/20/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8BHadoop(HDFS)/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据技术之Hadoop-HDFS"><a href="#大数据技术之Hadoop-HDFS" class="headerlink" title="大数据技术之Hadoop(HDFS)"></a>大数据技术之Hadoop(HDFS)</h1><h2 id="第一章-HDFS概述"><a href="#第一章-HDFS概述" class="headerlink" title="第一章 HDFS概述"></a>第一章 HDFS概述</h2><h4 id="1-1HDFS产生背景及定义"><a href="#1-1HDFS产生背景及定义" class="headerlink" title="1.1HDFS产生背景及定义"></a>1.1HDFS产生背景及定义</h4><p><strong>1.1.1HDFS****产生背景</strong></p><p>随着数据量越来越大，在一个操作系统存不下所有的数据，那么就分配到更多的操作系统管理的磁盘中，但是不方便和维护，迫切需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统。HDFS只是分布式文件管理系统中的一种。</p><p><strong>1.1.2HDFS****定义</strong></p><p>HDFS(Hadoop Distributed File System)，它是一个分布式文件系统。用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。</p><p>HDFS的使用场景：适合一次写入，多次读出的场景，且不支持文件的修改。适合用来做数据分析，并不适合用来做网盘应用。</p><p><strong>1.2HDFS****的优缺点</strong></p><p><strong>1.2.1****优点</strong></p><ol><li>  高容错性</li></ol><p>(1)  数据自动保存多个副本，他通过增加副本的形式，提高容错性。</p><p>(2)  某一个副本丢失以后，它可以自动恢复。</p><ol start="2"><li>  适合处理大数据</li></ol><p>(1)  数据规模：能够处理数据规模达到DB、TB，甚至PB级别的数据</p><p>(2)  文件规模：能够处理百万规模以上的文件数量，数量相当之大</p><ol start="3"><li>  可构建在廉价的机器上，通过多副本机制，提高可靠性</li></ol><p><strong>1.2.2****缺点</strong></p><ol><li><p>  不适合低延时数据访问，比如毫秒级的存储数据，是做不到的</p></li><li><p>  无法高效的对大量小文件进行存储</p></li></ol><p>(1)  存储大量小文件的话，它会占用NameNode大量的内存存储文件目录和块信息。这样是不可取的，因为NameNode的内存是有限的</p><p>(2)  小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标</p><ol start="3"><li>  不支持并发写入、文件随机修改</li></ol><p>(1)  一个文件只能有一个写，不允许多个线程同时写；</p><p>(2)  仅支持append（追加），不支持文件的随机修改</p><h4 id="1-3HDFS组成架构"><a href="#1-3HDFS组成架构" class="headerlink" title="1.3HDFS组成架构"></a>1.3HDFS组成架构</h4><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591154578987-d563a34b-ef01-48f7-a9f7-ba6ed3baf10c.png" alt="image.png"></p><ol><li> NameNode：就是Master，它是一个主管，管理者。</li></ol><p>(1)   管理HDFS的名称空间</p><p>(2)   配置副本策略</p><p>(3)   管理数据块（Block）映射信息；</p><p>(4)   处理客户端读写请求</p><ol start="2"><li> DataNode：就是Slave。NameNode下达命令，DataNode执行实际的操作。</li></ol><p>(1)   存储实际的数据块</p><p>(2)   执行数据块的读/写读写</p><ol start="3"><li> Client：就是客户端</li></ol><p>(1)   文件切分，文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传</p><p>(2)   与NameNode交互，获取文件的位置信息</p><p>(3)   与DataNode交互，获取文件的数据</p><p>(4)   Client提供一些命令来管理HDFS，比如NameNode格式化</p><p>(5)   Client可以通过一些命令来访问HDFS，比如对HDFS增删改查操作</p><ol start="4"><li> SecondaryNameNode：并非NameNode的热备，当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。</li></ol><p>(1)   辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode</p><p>(2)   在紧急情况下，可辅助恢复NameNode</p><h4 id="1-4HDFS文件块大小"><a href="#1-4HDFS文件块大小" class="headerlink" title="1.4HDFS文件块大小"></a>1.4HDFS文件块大小</h4><p>HDFS中的文件在物理上是分块存储的（Block）,块的大小可以通过配置参数（dfs.blocksize）来规定，默认大小在Hadoop2x版本中是128M，老版本是64M。</p><p><strong>HDFS****的寻址时间与传输时间</strong></p><p>(1)  如果寻址时间约为10ms，即查找到目标block的时间为10ms</p><p>(2)  寻址时间为传输时间的1%时，则为最佳状态。因此，传输时间=10ms/0.01=1000ms=1s</p><p>(3)  目前磁盘传输速度普遍为100MB/s</p><p><strong>思考：为什么块的大小不能设置太小，也不能设置太大。</strong></p><p>(1)  HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置</p><p>(2)  如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢</p><p><strong>总结：HDFS****块大小设置主要取决于磁盘传输速率</strong></p><h2 id="第二章-HDFS的shell操作"><a href="#第二章-HDFS的shell操作" class="headerlink" title="第二章 HDFS的shell操作"></a>第二章 HDFS的shell操作</h2><h4 id="2-1基本语法"><a href="#2-1基本语法" class="headerlink" title="2.1基本语法"></a>2.1基本语法</h4><p>$HADOOP_HOME/bin/hadoop fs 具体命令 或 $HADOOP_HOME/bin/hdfs dfs 具体命令</p><p>执行的都是 org.apache.hadoop.fs.FsShell</p><h4 id="2-2命令大全"><a href="#2-2命令大全" class="headerlink" title="2.2命令大全"></a>2.2命令大全</h4><p>[-appendToFile <localsrc> … <dst>]</p><p>​    [-cat [-ignoreCrc] <src> …]</p><p>​    [-checksum <src> …]</p><p>​    [-chgrp [-R] GROUP PATH…]</p><p>​    [-chmod [-R] &lt;MODE[,MODE]… | OCTALMODE&gt; PATH…]</p><p>​    [-chown [-R] [OWNER][:[GROUP]] PATH…]</p><p>​    [-copyFromLocal [-f] [-p] <localsrc> … <dst>]</p><p>​    [-copyToLocal [-p] [-ignoreCrc] [-crc] <src> … <localdst>]</p><p>​    [-count [-q] <path> …]</p><p>​    [-cp [-f] [-p] <src> … <dst>]</p><p>​    [-createSnapshot <snapshotDir> [<snapshotName>]]</p><p>​    [-deleteSnapshot <snapshotDir> <snapshotName>]</p><p>​    [-df [-h] [<path> …]]</p><p>​    [-du [-s] [-h] <path> …]</p><p>​    [-expunge]</p><p>​    [-get [-p] [-ignoreCrc] [-crc] <src> … <localdst>]</p><p>​    [-getfacl [-R] <path>]</p><p>​    [-getmerge [-nl] <src> <localdst>]</p><p>​    [-help [cmd …]]</p><p>​    [-ls [-d] [-h] [-R] [<path> …]]</p><p>​    [-mkdir [-p] <path> …]</p><p>​    [-moveFromLocal <localsrc> … <dst>]</p><p>​    [-moveToLocal <src> <localdst>]</p><p>​    [-mv <src> … <dst>]</p><p>​    [-put [-f] [-p] <localsrc> … <dst>]</p><p>​    [-renameSnapshot <snapshotDir> <oldName> <newName>]</p><p>​    [-rm [-f] [-r|-R] [-skipTrash] <src> …]</p><p>​    [-rmdir [–ignore-fail-on-non-empty] <dir> …]</p><p>​    [-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[–set <acl_spec> <path>]]</p><p>​    [-setrep [-R] [-w] <rep> <path> …]</p><p>​    [-stat [format] <path> …]</p><p>​    [-tail [-f] <file>]</p><p>​    [-test -[defsz] <path>]</p><p>​    [-text [-ignoreCrc] <src> …]</p><p>​    [-touchz <path> …]</p><p>​    [-usage [cmd …]]</p><p>Hadoop命令分类：</p><p>(1) 本地——》HDFS</p><p>-put</p><p>-copyFromLocal</p><p>-moveFromLocal</p><p>-appendToFile</p><p>(2) HDFS——》HDFS</p><p>-cp</p><p>-mv</p><p>-chown</p><p>-mkdir</p><p>-du</p><p>-rm</p><p>….</p><p>(3) HDFS——》本地</p><p>-get</p><p>-getmerge</p><p>-copyToLocal</p><h4 id="2-3常用命令实操"><a href="#2-3常用命令实操" class="headerlink" title="2.3常用命令实操"></a>2.3常用命令实操</h4><p>(1) 动Hadoop集群</p><p>[root@hadoop01 hadoop-2.8.5] sbin/start-dfs.sh</p><p>(2) -help：输出这个命令参数</p><p>[root@hadoop01 hadoop-2.8.5] hadoop fs -help rm</p><p>(3) -ls：显示目录信息</p><p>[root@hadoop01 hadoop-2.8.5] hadoop fs -ls /</p><p>(4) -mkdir：在HDFS上显示目录</p><p>[root@hadoop01 hadoop-2.8.5] hadoop fs -mkdir -p /abc/a</p><p>(5) -moveFromLocal：从本地剪切粘贴到HDFS</p><p>[root@hadoop01 hadoop-2.8.5] touch kongming.txt</p><p>[root@hadoop01 hadoop-2.8.5] hadoop fs -moveFromLocal ./abc.txt /abc/a</p><p>(6) -appendToFile：追加一个文件到已经存在的文件末尾</p><p>[root@hadoop01 hadoop-2.8.5] touch xiaoqiao.txt</p><p>[root@hadoop01 hadoop-2.8.5] vi xiaoqiao.txt</p><p>输入</p><p>Wangzherongyao</p><p>[root@hadoop01 hadoop-2.8.5] hadoop fs -appendToFile xiaoqiao.txt /abc/a/abc.txt</p><p>(7) -cat：显示文件内容</p><p>[root@hadoop01 hadoop-2.8.5] hadoop fs -cat /abc/a/abc.txt</p><p>(8) -chgrp(改变文件所属组)、-chmod(修改文件权限)、-chown(改变文件所有者)：linux文件系统中的用法一样，修改文件所属权限</p><p>[root@hadoop01 hadoop-2.8.5] hadoop fs -chmod 666 /abc/a/abc.txt</p><p>[root@hadoop01 hadoop-2.8.5] hadoop fs -chown root:root /abc/a/abc.txt</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591154810647-cbf48dbe-07d8-4d46-9f8d-bf75ebf5b773.png" alt="image.png"></p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591154803917-855371a8-26f3-4993-8da5-25ea6b0bcbf4.png" alt="image.png"></p><p>(9) -copyFromLocal：从本地文件系统中拷贝文件到HDFS路径中</p><p>[root@hadoop01 hadoop-2.8.5] hadoop fs -copyFromLocal start.sh /</p><p>(10)  -copyToLocal：从HDFS拷贝到本地</p><p>[root@hadoop01 hadoop-2.8.5] hadoop fs -copyToLocal /abc/a/abc.txt ./</p><p>(11)  -cp：从HDFS的一个路径拷贝到HDFS的另一个路径</p><p>[root@hadoop01 hadoop-2.8.5] hadoop fs -cp /abc/a/abc.txt /c.txt</p><p>会报异常，因为名字给更改了，如果不想报异常，可以只给目录，不修改名字</p><p>[root@hadoop01 hadoop-2.8.5] hadoop fs -cp /abc/a/abc.txt /</p><p>(12)  -mv：从HDFS目录中移动文件</p><p>[root@hadoop01 hadoop-2.8.5] hadoop fs -mv /c.txt /abc/a/ </p><p>(13)  -get：等同于copyToLocal，就是从HDFS下载文件到本地</p><p>[root@hadoop01 hadoop-2.8.5] hadoop fs -get /abc/a/c.txt ./</p><p>(14)  -getmerge：合并下载多个文件，比如HDFS的目录/abc/下有多个文件：log.1，log.2，log.3…</p><p>[root@hadoop01 hadoop-2.8.5] hadoop fs -getmerge /abc/a/* ./zaiyiqi.txt</p><p>(15)  -put：等同于copyFromLocal，从本地文件系统中拷贝文件到HDFS路径中</p><p>[root@hadoop01 hadoop-2.8.5] hadoop fs -put ./zaiyiqi.txt /abc/a/</p><p>(16)  -tail：显示一个文件的末尾</p><p>实时监控文件<img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591154650256-f924a9ac-9e5a-4952-b886-022f9c8d6eed.png" alt="image.png"></p><p>[root@hadoop01 hadoop-2.8.5] hadoop fs -tail -f /abc/a/abc.txt</p><p>往 /abc/a/abc.txt追加内容</p><p>[root@hadoop01 hadoop-2.8.5] hadoop fs -appendToFile start.sh /abc/a/abc.txt</p><p>(17)  -rm：删除文件或文件夹</p><p>[root@hadoop01 hadoop-2.8.5] hadoop fs -rm /abc/a/c.txt</p><p>[root@hadoop01 hadoop-2.8.5] hadoop fs -rm -r /abc/a/</p><p>(18)  -rmdir：删除空目录</p><p>[root@hadoop01 hadoop-2.8.5] hadoop fs -mkdir /test</p><p>[root@hadoop01 hadoop-2.8.5] hadoop fs -rmdir /test</p><p>(19)  -du统计文件夹的大小信息</p><p>[root@hadoop01 hadoop-2.8.5] hadoop fs -du -s -h /abc/a/</p><p>256 /abc/a</p><p>[root@hadoop01 hadoop-2.8.5] hadoop fs -du -h /abc/a/</p><p>211 /abc/a/abc.txt</p><p>15  /abc/a/c.txt</p><p>30  /abc/a/zaiyiqi.txt</p><p>(20)  -setrep：设置HDFS中文件的副本数量</p><p>[root@hadoop01 hadoop-2.8.5] hadoop fs -setrep 10 /sanguo/shuguo/kongming.txt</p><p>这里设置的副本数只是记录在NameNode的元数据中，是否真的会有这么多副本，还是得看DataNode的数量，因为目前只有三台设备，最多也就3个副本，只有节点数的增加到10台时，副本数量才能达到10。</p><h2 id="第三章-HDFS的客户端操作"><a href="#第三章-HDFS的客户端操作" class="headerlink" title="第三章 HDFS的客户端操作"></a>第三章 HDFS的客户端操作</h2><h4 id="3-1客户端环境准备-提前配置java环境，jdk"><a href="#3-1客户端环境准备-提前配置java环境，jdk" class="headerlink" title="3.1客户端环境准备(提前配置java环境，jdk)"></a>3.1客户端环境准备(提前配置java环境，jdk)</h4><p>\1.   根据自己电脑的操作系统拷贝对应的编译后的hadoop jar包到非中文路径（例如：F:\environment\hadoop），如图所示。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591154656276-8d6683d6-6b9c-4cc6-89b5-324d6208f9b3.png" alt="image.png"></p><p>\2.   配置HADOOP_HOME环境变量</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591154673008-6f523f11-4773-42f5-97fb-9518de099c06.png" alt="image.png"></p><p>\3.   配置Path环境变量</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591154757246-8e388929-81fe-4cd5-8a10-982fc8a0f4bb.png" alt="image.png"></p><p>\4.   将hadoop中的jar包导入到eclipse中</p><p>4.1</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591154681743-12121ebf-b0fd-4de2-866e-c10d5026a4ae.png" alt="image.png"></p><p>4.2</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591154691065-57d06b6b-2e6b-4b89-8515-946a99891d4c.png" alt="image.png"></p><p>4.3进入hadoop公用的jar包文件夹,这里面的jar包是共同使用的,所以进行导入</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591154698768-3d447a85-82db-4789-9528-3fa4e967fc18.png" alt="image.png"></p><p>4.4先将红框中的jar包导入,然后再进入lib中,将所有的jar包导入,因为红框中的jar包肯定还有很多依赖,避免出现错误,最好是将lib中的jar包全部导入</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591154707754-b65a910c-ac0f-43fd-81d4-5e9d0649aafe.png" alt="image.png"></p><p>4.5因为使用的hafs所以肯定要将hdfs的jar包也导入到eclipse</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591154715066-5031c8d7-b1c1-4031-937f-0859aea2c92b.png" alt="image.png"></p><p>4.6最好将红框中的jar包全部导入,这样可以避免因缺少jar包而出现程序的错误,同样,红框中的jar包肯定也有很多依赖,所以将lib中的jar包也要全部进行导入,在导入的时候会提示有重复的jar包,直接覆盖就行</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591154723124-ae5f40d4-2a63-4470-be66-208b3493585d.png" alt="image.png"></p><p>到这里jar包就导入完毕了,可以进行程序的编写了</p><h4 id="3-2入门示例"><a href="#3-2入门示例" class="headerlink" title="3.2入门示例"></a>3.2入门示例</h4><p><strong>package</strong> com.bigData; <strong>import</strong> java.net.URI;<strong>import</strong> org.apache.hadoop.conf.Configuration;<strong>import</strong> org.apache.hadoop.fs.FileSystem;<strong>import</strong> org.apache.hadoop.fs.Path; <strong>public</strong> <strong>class</strong>  Demo {   <strong>public</strong> <strong>static</strong> <strong>void</strong> main(String[] args) <strong>throws</strong> Exception {    //构造一个配置参数封装对象，封装一些必要的参数    Configuration  conf = <strong>new</strong>  Configuration();    //表明当前程序端默认的操作文件系统    conf.set(“fs.defaultFS”, “hdfs://192.168.88.11:9000/“);    //设置切块的大小    conf.set(“dfs.blocksize”, “64m”);    //设置切块的副本数量    conf.setInt(“dfs.replication”, 2);    /<em>//先获取一个hdfs客户端访问对象，未指明客户端的用户身份，api会从当前系统中获取当前系统作为访问hdfs对象    FileSystem  fs = FileSystem.get(conf);</em>/    //显示的指明访问hdfs客户端用户是root    FileSystem fs =  FileSystem.<em>get</em>(<strong>new</strong> URI(“hdfs://192.168.88.11:9000/“), conf, “root”);    //用客户端对象进行操作，上传文件到hdfs中    fs.copyFromLocalFile(<strong>new</strong> Path(“C:\Users\Administrator\Desktop\Hadoop\HDFS\HDFS.docx”), <strong>new</strong>  Path(“/“));    //关闭客户端对象    fs.close();  }}</p><h4 id="3-3HDFS的API操作"><a href="#3-3HDFS的API操作" class="headerlink" title="3.3HDFS的API操作"></a>3.3HDFS的API操作</h4><h5 id="3-3-1初始化和关闭连接操作"><a href="#3-3-1初始化和关闭连接操作" class="headerlink" title="3.3.1初始化和关闭连接操作"></a>3.3.1初始化和关闭连接操作</h5><p>FileSystem fs ;    /**   * 初始化配置   * <strong>@throws</strong> Exception   <em>/  @Before  <strong>public</strong> <strong>void</strong>  init() <strong>throws</strong>  Exception {    URI uri = <strong>new</strong> URI(“hdfs://192.168.88.11:9000/“);    Configuration  conf = <strong>new</strong>  Configuration();        String user = <strong>new</strong> String(“root”);    conf.set(“fs.defaultFS”, “hdfs://192.168.88.11:9000/“);        fs =  FileSystem.<em>get</em>(uri, conf, user);  } /*</em>   * 关闭客户端对象   * <strong>@throws</strong> Exception   */  @After  <strong>public</strong> <strong>void</strong>  closeTest() <strong>throws</strong>  Exception {    fs.close();  } </p><h5 id="3-3-2创建文件夹"><a href="#3-3-2创建文件夹" class="headerlink" title="3.3.2创建文件夹"></a>3.3.2创建文件夹</h5><p>/**   * 创建文件夹   * <strong>@throws</strong> Exception   */  @Test  <strong>public</strong> <strong>void</strong>  mkdirTest() <strong>throws</strong>  Exception {            <strong>boolean</strong> b = fs.mkdirs(<strong>new</strong>  Path(“/eclipse”));        System.*<strong>out*</strong>.println(b);      }</p><h5 id="3-3-3上传文件"><a href="#3-3-3上传文件" class="headerlink" title="3.3.3上传文件"></a>3.3.3上传文件</h5><p>/**   * 上传文件   *   * <strong>@throws</strong> Exception    */  @Test  <strong>public</strong> <strong>void</strong>  putTest() <strong>throws</strong>  Exception {    fs.copyFromLocalFile(<strong>new</strong> Path(“D:\hello.txt”), <strong>new</strong>  Path(“/eclipse/“));    }</p><h5 id="3-3-4下载文件"><a href="#3-3-4下载文件" class="headerlink" title="3.3.4下载文件"></a>3.3.4下载文件</h5><p>/**   * 下载文件   * <strong>@throws</strong> Exception    * <strong>@throws</strong> IllegalArgumentException    */  <strong>public</strong> <strong>void</strong>  getTest() <strong>throws</strong> Exception {    fs.copyToLocalFile(<strong>new</strong> Path(“/eclipse/hello.txt”), <strong>new</strong>  Path(“E:\“));  }</p><h5 id="3-3-5移动文件或更改名字"><a href="#3-3-5移动文件或更改名字" class="headerlink" title="3.3.5移动文件或更改名字"></a>3.3.5移动文件或更改名字</h5><p>/**   * 移动文件或更改名字   * <strong>@throws</strong> Exception    * <strong>@throws</strong> IllegalArgumentException    */  @Test  <strong>public</strong> <strong>void</strong>  moveTest() <strong>throws</strong> Exception {    <strong>boolean</strong> b = fs.rename(<strong>new</strong> Path(“/a.txt”), <strong>new</strong>  Path(“/eclipse/“));    System.*<strong>out*</strong>.println(b);  }</p><h5 id="3-3-6删除文件"><a href="#3-3-6删除文件" class="headerlink" title="3.3.6删除文件"></a>3.3.6删除文件</h5><p>/**   * 删除文件   * <strong>@throws</strong> IOException    * <strong>@throws</strong> IllegalArgumentException    */  @Test  <strong>public</strong> <strong>void</strong>  delTest() <strong>throws</strong>  Exception {    //第二个参数表示是否要递归删除    <strong>boolean</strong> b = fs.delete(<strong>new</strong>  Path(“/a1.txt”),<strong>false</strong>);        System.*<strong>out*</strong>.println(b);  }</p><h5 id="3-3-7查看目录信息-全是文件信息，不返回文件夹信息"><a href="#3-3-7查看目录信息-全是文件信息，不返回文件夹信息" class="headerlink" title="3.3.7查看目录信息(全是文件信息，不返回文件夹信息)"></a>3.3.7查看目录信息(全是文件信息，不返回文件夹信息)</h5><p>/**   * 查看目录信息(全是文件信息，不返回文件夹信息)   * <strong>@throws</strong> Exception    * <strong>@throws</strong> IllegalArgumentException    * <strong>@throws</strong> FileNotFoundException    */  @Test  <strong>public</strong> <strong>void</strong>  lsTest() <strong>throws</strong> Exception  {    //对二个参数是否递归目录信息    RemoteIterator<LocatedFileStatus>  lfs = fs.listFiles(<strong>new</strong>  Path(“/eclipse”), <strong>true</strong>);        <strong>while</strong>(lfs.hasNext()){      LocatedFileStatus  next = lfs.next();      System.<em><strong>out*</strong>.println(“文件的全路径：”+next.getPath());      System.<em><strong>out*</strong>.println(“最近访问时间：”+<strong>new</strong> Date(next.getAccessTime()));      System.<em><strong>out*</strong>.println(“文件大小的为：”+next.getLen());      System.<em><strong>out*</strong>.println(“文件块大小的为：”+next.getBlockSize());            System.<em><strong>out*</strong>.println(“文件块存放的位置：”+Arrays.<em>toString</em>(next.getBlockLocations()));      System.<em><strong>out*</strong>.println(“<em><strong><strong><strong>快信息</strong></strong></strong></em></em></em></em></em></em>**</em>“);      BlockLocation[]  blockLocations = next.getBlockLocations();      <strong>for</strong> (BlockLocation b : blockLocations) {       System.*<strong>out*</strong>.println(“本块副本所在datanode主机：”+Arrays.<em>toString</em>(b.getHosts()));       System.*<strong>out*</strong>.println(“本块的大小：”+b.getLength());       System.*<strong>out*</strong>.println(“本块在整个文件中的起始偏移量：”+b.getOffset());      }    }      }</p><h5 id="3-3-8查看文件夹信息"><a href="#3-3-8查看文件夹信息" class="headerlink" title="3.3.8查看文件夹信息"></a>3.3.8查看文件夹信息</h5><p>/**   * 查看文件夹信息   * <strong>@throws</strong> IOException    * <strong>@throws</strong> IllegalArgumentException    * <strong>@throws</strong> FileNotFoundException    <em>/  @Test  <strong>public</strong> <strong>void</strong>  lsTest2() <strong>throws</strong>  Exception {    FileStatus[]  ls = fs.listStatus(<strong>new</strong>  Path(“/“));    <strong>for</strong> (FileStatus fs : ls) {      System.<em><strong>out*</strong>.println(fs.isDirectory()?”文件夹”:”文件”);      System.<strong>*out***.println(“文件夹或者文件的全路径：”+fs.getPath());      System.*<strong>out*</strong>.println(“</strong></em>************</em>************”);    }      }</p><h5 id="3-3-9读取hdfs中的文件"><a href="#3-3-9读取hdfs中的文件" class="headerlink" title="3.3.9读取hdfs中的文件"></a>3.3.9读取hdfs中的文件</h5><p>/**   * 读取hdfs中的文件   * <strong>@throws</strong> IOException    * <strong>@throws</strong> IllegalArgumentException    */  @Test  <strong>public</strong> <strong>void</strong>  readFileTest() <strong>throws</strong>  Exception {    FSDataInputStream  fis = fs.open(<strong>new</strong>  Path(“/abc.txt”));        BufferedReader  br = <strong>new</strong>  BufferedReader(<strong>new</strong>  InputStreamReader(fis));      String len = <strong>null</strong>;    <strong>while</strong>((len=br.readLine())!=<strong>null</strong>) {      System.*<strong>out*</strong>.println(len);    }        br.close();    fis.close();  }</p><h5 id="3-3-10向hdfs文件写入内容"><a href="#3-3-10向hdfs文件写入内容" class="headerlink" title="3.3.10向hdfs文件写入内容"></a>3.3.10向hdfs文件写入内容</h5><p>/**   * 向hdfs文件写入内容   * <strong>@throws</strong> IOException    * <strong>@throws</strong> IllegalArgumentException    */  @Test  <strong>public</strong> <strong>void</strong>  writeFileTest() <strong>throws</strong>  Exception {        FSDataOutputStream  fos = fs.append(<strong>new</strong>  Path(“/abc.txt”));        BufferedWriter  bw = <strong>new</strong>  BufferedWriter(<strong>new</strong>  OutputStreamWriter(fos));        bw.write(“hello”);        bw.close();    fos.close();  }</p><h4 id="3-4WordCount案例（作业）"><a href="#3-4WordCount案例（作业）" class="headerlink" title="3.4WordCount案例（作业）"></a>3.4WordCount案例（作业）</h4><p><strong>需求</strong></p><p><strong>1.</strong>  <strong>把本地文件上传到hdfs</strong></p><p><strong>2.</strong>  <strong>进行读取分析统计</strong></p><p><strong>3.</strong>  <strong>统计好的结果写入hdfs****文件中</strong></p><p><strong>4.</strong>  <strong>查看hdfs****中的结果</strong></p><h1 id="大数据技术之Hadoop-MapReduce"><a href="#大数据技术之Hadoop-MapReduce" class="headerlink" title="大数据技术之Hadoop(MapReduce)"></a>大数据技术之Hadoop(MapReduce)</h1><h1 id="第1章-MapReduce概述"><a href="#第1章-MapReduce概述" class="headerlink" title="第1章 MapReduce概述"></a>第1章 MapReduce概述</h1><h2 id="1-1-MapReduce定义"><a href="#1-1-MapReduce定义" class="headerlink" title="1.1 MapReduce定义"></a>1.1 MapReduce定义</h2><p>MapReduce是一个分布式运算程序的编程框架，是用户开发“基于Hadoop的数据分析应用”的核心框架。</p><p>MapReduce核心功能是讲用户编写的业务逻辑代码和自带默认组件整个成一个完整的分布式运算程序，并发运行在Hadoop集群上。</p><h2 id="1-2-MapReduce优缺点"><a href="#1-2-MapReduce优缺点" class="headerlink" title="1.2 MapReduce优缺点"></a>1.2 MapReduce优缺点</h2><h4 id="1-2-1优点"><a href="#1-2-1优点" class="headerlink" title="1.2.1优点"></a>1.2.1优点</h4><p><strong>(1)</strong>   <strong>MapReduce****易于编程</strong></p><p>它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行。</p><p><strong>(2)</strong>   <strong>良好的扩展性</strong></p><p>当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。</p><p><strong>(3)</strong>   <strong>高容错性</strong></p><p>MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如，其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而且过程不需要人工参与，而完全是由Hadoop内部完成的。</p><p><strong>(4)</strong>   <strong>适合PB****级以上海量数据的离线处理</strong></p><p>可以实现上千台服务器集群并发工作，提供数据处理能力。</p><h4 id="1-2-2缺点"><a href="#1-2-2缺点" class="headerlink" title="1.2.2缺点"></a>1.2.2缺点</h4><p>(1)   不擅长实时计算</p><p>MapReudce无法像MySQL一样，在毫秒或者秒级内返回结果</p><p>(2)   不擅长流式计算</p><p>流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化，这是因为MapReduce自身的设计特点决定了数据源必须是静态的。</p><p>(3)   不擅长DAG(有向图)计算</p><p>多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出，在这种情况</p><p>下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写</p><p>到磁盘，会造成大量的磁盘IO，导致性能非常的底下。</p><h2 id="1-3MapReduce核心思想"><a href="#1-3MapReduce核心思想" class="headerlink" title="1.3MapReduce核心思想"></a>1.3MapReduce核心思想</h2><h4 id="1-3-1MapReduce是什么呢？"><a href="#1-3-1MapReduce是什么呢？" class="headerlink" title="1.3.1MapReduce是什么呢？"></a>1.3.1MapReduce是什么呢？</h4><p>MapReduce首先是一种运算模型</p><p>它将一个数据运算逻辑分为两个阶段：</p><p>map阶段：读取原始数据，映射成key-value</p><p>Reduce阶段：将map产生的key-value按key在分组聚合！</p><p>这种思想的优点：根据这种模型开发的运算程序很容易实现分布式运算！</p><p>Hadoop就根据这个思想，用java实现了一套编程框架：名叫mapredue</p><p>Spark就根据这个思想，用scala实现了一套封装更好的编程框架：名叫spark</p><h4 id="1-3-2MapReduce怎么工作的？"><a href="#1-3-2MapReduce怎么工作的？" class="headerlink" title="1.3.2MapReduce怎么工作的？"></a>1.3.2MapReduce怎么工作的？</h4><p>分布式运算的核心基本套路：</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591154916223-c8c11299-62e3-40c9-84c8-2aa71acaca48.png" alt="image.png"></p><h1 id="第2章WordCount案例实操"><a href="#第2章WordCount案例实操" class="headerlink" title="第2章WordCount案例实操"></a>第2章WordCount案例实操</h1><h2 id="2-1需求"><a href="#2-1需求" class="headerlink" title="2.1需求"></a>2.1需求</h2><p>在给定的文本文件中统计输出每一个单词出现的总次数</p><h2 id="2-2创建工程"><a href="#2-2创建工程" class="headerlink" title="2.2创建工程"></a>2.2创建工程</h2><p>(1) 创建maven项目</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591154933498-ab9afe7b-bb4e-49aa-97ae-521bc0a4cfe8.png" alt="image.png"></p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591154948405-4a0e504f-0b1f-4ad1-ab5a-aa617cff12b7.png" alt="image.png"></p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591154968179-6b9f9e41-5254-4b6e-a9db-a6118c0f330a.png" alt="image.png"></p><p>(2) 在pom.xml文件中添加依赖</p><pre class="line-numbers language-none"><code class="language-none">&lt;**dependencies**&gt;     *&lt;!--  &lt;dependency&gt;          &lt;groupId&gt;junit&lt;&#x2F;groupId&gt;          &lt;artifactId&gt;junit&lt;&#x2F;artifactId&gt;          &lt;version&gt;RELEASE&lt;&#x2F;version&gt;        &lt;&#x2F;dependency&gt;--&gt;    &lt;!-- &lt;dependency&gt;       &lt;groupId&gt;org.apache.logging.log4j&lt;&#x2F;groupId&gt;       &lt;artifactId&gt;log4j-core&lt;&#x2F;artifactId&gt;       &lt;version&gt;2.8.2&lt;&#x2F;version&gt;     &lt;&#x2F;dependency&gt;--&gt; *    &lt;**dependency**&gt;      &lt;**groupId**&gt;org.apache.hadoop&lt;&#x2F;**groupId**&gt;      &lt;**artifactId**&gt;hadoop-common&lt;&#x2F;**artifactId**&gt;      &lt;**version**&gt;2.8.1&lt;&#x2F;**version**&gt;    &lt;&#x2F;**dependency**&gt;    &lt;**dependency**&gt;      &lt;**groupId**&gt;org.apache.hadoop&lt;&#x2F;**groupId**&gt;      &lt;**artifactId**&gt;hadoop-client&lt;&#x2F;**artifactId**&gt;      &lt;**version**&gt;2.8.1&lt;&#x2F;**version**&gt;    &lt;&#x2F;**dependency**&gt;    &lt;**dependency**&gt;      &lt;**groupId**&gt;org.apache.hadoop&lt;&#x2F;**groupId**&gt;      &lt;**artifactId**&gt;hadoop-hdfs&lt;&#x2F;**artifactId**&gt;      &lt;**version**&gt;2.8.1&lt;&#x2F;**version**&gt;    &lt;&#x2F;**dependency**&gt;  &lt;&#x2F;**dependencies**&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="2-3编写程序"><a href="#2-3编写程序" class="headerlink" title="2.3编写程序"></a>2.3编写程序</h2><h4 id="2-3-1Mapper类"><a href="#2-3-1Mapper类" class="headerlink" title="2.3.1Mapper类"></a>2.3.1Mapper类</h4><pre class="line-numbers language-none"><code class="language-none">*&#x2F;**  \* KEYIN**：maptask传入的key的类型（此处的key是maptask所读到的一行数据的起始偏移量），long  \* VALUEIN：maptask传入的value的类型（此处的value是maptask所读到的一行数据的内容），String  \*  \* KEYOUT：我们的map方法返回的key数据的类型（此处是一个单词）,String  \* VALUEOUT：我们的map方法返回的value数据的类型（此处是一个1）,Integer  \*  \*  \* 但是，在mapreduce框架内，数据需要在maptask和reducetask之间进行网络传输以及需要写磁盘 \* key-value数据就需要经常序列化和反序列化 \* 为了提高效率，hadoop自己开发了一套序列化机制（Writable接口），没有使用jdk原生的序列化机制 \* 所以不能直接使用上述的jdk原生类型，而是要用以下经过hadoop改造的，实现了Writable接口的类型： \* String --&gt; Text  \* Long --&gt; LongWritable  \* Integer --&gt; IntWritable  \* Float --&gt; FloatWritable  \* Double --&gt; DoubleWritable  \* ....  \*&#x2F;*  **class** WordcountMapper **extends** Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123;   *&#x2F;&#x2F; maptask**程序调用map方法   &#x2F;&#x2F; maptask每读取一行数据调用一次map方法   &#x2F;&#x2F; maptask调用map方法时，会将这一行的起始偏移量传入参数key, 会将这一行的内容传入参数value*   @Override   **protected void** map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, IntWritable&gt;.Context context)       **throws** IOException, InterruptedException &#123;     *&#x2F;&#x2F; hello tom hello jim ....     &#x2F;&#x2F;* *将这一行数据按照空格切分出单词*     String[] words &#x3D; value.toString().split(**&quot; &quot;**);     *&#x2F;&#x2F;* *将每一个单词拼成（单词，1）返回*     **for** (String word : words) &#123;       context.write(**new** Text(word), **new** IntWritable(1));     &#125;   &#125; &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="2-3-2Reducer类"><a href="#2-3-2Reducer类" class="headerlink" title="2.3.2Reducer类"></a>2.3.2Reducer类</h4><pre class="line-numbers language-none"><code class="language-none">*&#x2F;**  \*  \* KEYIN:reduce task**传入的key数据的类型，与map端输出的key的类型一致，Text  \* VALUEIN:reduce task传入的value数据的类型，与map端输出的value的类型一致，IntWritable  \*  \* KEYOUT：我们的reduce方法将要返回的key数据的类型(此案例中是一个单词)，Text  \* VALUEOUT：我们的reduce方法将要返回的value数据的类型（此案例中是一个总数），IntWritable  \*&#x2F;*  **class** WordcountReducer **extends** Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123;   *&#x2F;&#x2F; reduce**方法是谁会调？ —— reduce task程序   &#x2F;&#x2F; reduce task是如何调用reduce方法？   &#x2F;&#x2F;  它会将自己拿到的分区数据按照key分组，然后对每一组kv数据调用一次reduce方法   &#x2F;&#x2F;  它会将一组数据的key传入参数key，一组数据的所有value传入参数values   &#x2F;&#x2F; （hello,1）（hello,1）（hello,1）（hello,1）（hello,1）*   @Override   **protected void** reduce(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, &#96;&#96;IntWritable&gt;.Context context) **throws** IOException, InterruptedException &#123;     *&#x2F;&#x2F;* *累加这一组数据的所有value*     **int** count &#x3D; 0;     **for** (IntWritable value : values) &#123;       count +&#x3D; value.get();     &#125;     *&#x2F;&#x2F;* *返回运算结果*     context.write(key, **new** IntWritable(count));   &#125; &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="2-3-3启动类"><a href="#2-3-3启动类" class="headerlink" title="2.3.3启动类"></a>2.3.3启动类</h4><pre class="line-numbers language-none"><code class="language-none">**public class** Driver &#123;     **public static void** main(String[] args) **throws** Exception &#123;       *&#x2F;&#x2F;* *获取一个提交mr程序到yarn上去的客户端对象*       Configuration conf &#x3D; **new** Configuration();       Job job &#x3D; Job.*getInstance*(conf);       *&#x2F;&#x2F;* *提交mr任务之前，需要先设置这个mr任务的一些信息       &#x2F;&#x2F; 指定我们的mr程序所在的jar包路径       &#x2F;&#x2F;job.setJar(&quot;&#x2F;root&#x2F;wc.jar&quot;);*       job.setJarByClass(Driver.**class**); *&#x2F;&#x2F;**可以根据本类的类加载器获得本类所在jar包的路径       &#x2F;&#x2F; 指定Mapper类和Reducer类*       job.setMapperClass(WordcountMapper.**class**);       job.setReducerClass(WordcountReducer.**class**);       *&#x2F;&#x2F;* *指定我们的Mapper输出的key-value的类型*       job.setMapOutputKeyClass(Text.**class**);       job.setMapOutputValueClass(IntWritable.**class**);       *&#x2F;&#x2F;* *指定我们的reducer输出的key-value的类型*       job.setOutputKeyClass(Text.**class**);       job.setOutputValueClass(IntWritable.**class**);       *&#x2F;&#x2F;* *指定我们要处理的数据文件所在位置*       FileInputFormat.*setInputPaths*(job, **new** Path(**&quot;F:****\\****data****\\****wordcount****\\****input****\\****&quot;**));       *&#x2F;&#x2F;* *指定程序最后输出的结果文件所在位置(输出目录必须是不存在的目录，否则会抛异常)*       FileOutputFormat.*setOutputPath*(job, **new** Path(**&quot;&#x2F;F:****\\****data****\\****wordcount****\\****output****\\****&quot;**));       *&#x2F;&#x2F;* *指定本mr任务，reduce task的并行实例数*       job.setNumReduceTasks(2);       **boolean** res &#x3D; job.waitForCompletion(**true**); *&#x2F;&#x2F;**阻塞方法，一直会等待着mr程序在yarn集群中运行完毕*     &#125;   &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h1 id="第3章-MapperReduce框架原理详解"><a href="#第3章-MapperReduce框架原理详解" class="headerlink" title="第3章 MapperReduce框架原理详解"></a>第3章 MapperReduce框架原理详解</h1><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591155190613-ea924451-3412-4444-821e-595d45f65069.png" alt="image.png"></p><p>MapReduce中的每个map任务可以细分4个阶段：record reader、mapper、combiner和partitioner。map任务的输出被称为中间键和中间值，会被发送到reducer做后续处理。reduce任务可以分为4个阶段：混排（shuffle）、排序（sort）、reducer</p><p>和输出格式（output format）。map任务运行的节点会优先选择在数据所在的节点，因此，一般可以通过在本地机器上进行计算来减少数据的网络传输。</p><p>Mapreduce作业的输入是一系列存储在Hadoop分布式文件系统（HDFS）上的文件。在MapReduce中，这些文件通过输入格式（input format）被分成了一系列的输入split（input split）。输入split可以看做是文件在字节层面的分块表示，每个split由一个map任务负责处理。在一个reduce的调用过程中，定义一个comparator，对分组在一起的key进行排序。</p><p><strong>1.首先，在思考解决问题思路时，我们应该先深刻的理解MapReduce处理数据的整个流程，这是最基础的，不然的话是不可能找到解决问题的思路的。我描述一下MapReduce处理数据的大概流程：首先，MapReduce框架通过getSplits()方法实现对原始文件的切片之后，每一个切片对应着一个MapTask，InputSplit输入到map()函数进行处理，中间结果经过环形缓冲区的排序，然后分区、自定义二次排序(如果有的话)和合并，再通过Shuffle操作将数据传输到reduce Task端，reduce端也存在着缓冲区，数据也会在缓冲区和磁盘中进行合并排序等操作，然后对数据按照key值进行分组，然后每处理完一个分组之后就会去调用一次reduce()函数，最终输出结果。大概流程 我画了一下，如下图：</strong></p><p><img src="https://cdn.nlark.com/yuque/0/2020/jpeg/1489802/1591194099906-c531665c-cfaa-4dc4-bc42-e62faf7c00f7.jpeg" alt="image"></p><h1 id="第4章-Hadoop序列化接口"><a href="#第4章-Hadoop序列化接口" class="headerlink" title="第4章 Hadoop序列化接口"></a>第4章 Hadoop序列化接口</h1><h2 id="4-1序列化接口简单介绍"><a href="#4-1序列化接口简单介绍" class="headerlink" title="4.1序列化接口简单介绍"></a>4.1序列化接口简单介绍</h2><p>  Writable接口，也就是org.apache.hadoop.io.Writable接口。Hadoop的所有可序列化对象都必须实现这个接口。Writable接口里有两个方法，一个是write方法，将对象写入字节流，另一个是readFields方法，从字节流解析出对象。</p><p>  Java的API提供了Comparable接口，也就是java.lang.Comparable接口。这个接口只有一个方法，就是compareTo，用于比较两个对象。</p><p>  WritableComparable接口同时继承了Writable和Comparable这两个接口。</p><p>Hadoop里的三个类IntWritable、DoubleWritable和ByteWritable，都继承了WritableComparable接口。</p><p>注意，IntWritable、DoubleWritable和ByteWritable，尽管后缀是“Writable”，但它们不是接口，是类！！</p><p>​    Hadoop的序列化接口还有更多的类型，在这里不一一列举。</p><h2 id="4-2自定义bean对象实现序列化接口-Writable"><a href="#4-2自定义bean对象实现序列化接口-Writable" class="headerlink" title="4.2自定义bean对象实现序列化接口(Writable)"></a>4.2自定义bean对象实现序列化接口(Writable)</h2><p>在企业开发中往往常用的基本序列化类型不能满足所有需求，比如在Hadoop框架内部传递一个bean对象，那么该对象就需要实现序列化接口。</p><p>具体实现bean对象序列化步骤如下7步。</p><p>（1）必须实现Writable接口</p><p>（2）反序列化时，需要反射调用空参构造函数，所以必须有空参构造</p><p>（3）重写序列化方法(hadoop序列化该类的对象时，要调用的方法：将对象的属性值转成二进制)</p><p>@Override     public void  write(DataOutput out) throws IOException {         out.writeUTF(this.movie);         out.writeUTF(this.rate);         out.writeUTF(this.timeStamp);         out.writeUTF(this.uid);     }</p><p>（4）重写反序列化方法(hadoop在反序列化该类的对象时，要调用的方法：从二进制中解析出数据，赋给对象的属性)</p><p>@Override     public void  readFields(DataInput in) throws IOException {         this.movie  = in.readUTF();         this.rate  = in.readUTF();         this.timeStamp  = in.readUTF();         this.uid  = in.readUTF();      }</p><p>（5）注意反序列化的顺序和序列化的顺序完全一致</p><h2 id="4-3反序列化实操"><a href="#4-3反序列化实操" class="headerlink" title="4.3反序列化实操"></a>4.3反序列化实操</h2><h4 id="4-3-1需求：movie评分topn"><a href="#4-3-1需求：movie评分topn" class="headerlink" title="4.3.1需求：movie评分topn"></a>4.3.1需求：movie评分topn</h4><h4 id="4-3-2数据样例："><a href="#4-3-2数据样例：" class="headerlink" title="4.3.2数据样例："></a>4.3.2数据样例：</h4><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591155211380-777ebb8f-8d55-4557-9a21-0cadd7e18fab.png" alt="image.png"></p><p>K,movie</p><p>V:MovieBean </p><h4 id="4-3-3编写程序"><a href="#4-3-3编写程序" class="headerlink" title="4.3.3编写程序"></a>4.3.3编写程序</h4><h1 id="第3章-自定义分区"><a href="#第3章-自定义分区" class="headerlink" title="第3章 自定义分区"></a>第3章 自定义分区</h1><p>在Hadoop的MapReduce过程中，每个map task处理完数据后，如果存在自定义Combiner类，会先进行一次本地的reduce操作，然后把数据发送到Partitioner，由Partitioner来决定每条记录应该送往哪个reducer节点，默认使用的是HashPartitioner，其核心代码如下：</p><p><strong>public class HashPartitioner&lt;K, V&gt;  extends Partitioner&lt;K, V&gt; {</strong>  <strong>/**  Use {@link Object#hashCode()} to partition. */</strong>   <strong>public int getPartition(K key, V value,</strong>             <strong>int numReduceTasks)  {</strong>    <strong>return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</strong> <strong>}</strong> <strong>}</strong></p><p>getPartition函数的作用：</p><p>(1) 获取key的哈希值</p><p>(2) 使用key的哈希值对reduce任务数求模</p><h1 id="第4章-案例实操"><a href="#第4章-案例实操" class="headerlink" title="第4章 案例实操"></a>第4章 案例实操</h1><p>1、数据文件说明</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591167857167-d2bb8ba0-1be5-46b0-9962-90453f1cf2c3.png" alt="image.png"></p><p>这是一份温度检测数据，以文本形式存储。</p><p>日期和时间中间是空格，为整体，表示检测站点检测的时间。</p><p>后面是检测的温度，中间是Tab键隔开。</p><p>2、需求</p><p>每年的温度降序排序且每年单独一个文件输出存储</p><p>3、思路</p><p>(1)   按照每年升序排序再按照每年的温度降序排序</p><p>(2)   按照年份进行分组，每一年份对应一个reduce task</p><h1 id="第5章-安装MYSQL"><a href="#第5章-安装MYSQL" class="headerlink" title="第5章 安装MYSQL"></a>第5章 安装MYSQL</h1><h4 id="7-1上传rpm包解压"><a href="#7-1上传rpm包解压" class="headerlink" title="7.1上传rpm包解压"></a>7.1上传rpm包解压</h4><p> tar -xvf MySQL-5.6.26-1.linux_glibc2.5.x86_64.rpm-bundle.tar</p><p> <img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591167848385-b207ddbb-503f-45ab-a799-8e7d2a70ffd9.png" alt="image.png"></p><h4 id="7-2安装mysql的服务端与客户端"><a href="#7-2安装mysql的服务端与客户端" class="headerlink" title="7.2安装mysql的服务端与客户端"></a>7.2安装mysql的服务端与客户端</h4><p>rpm -ivh MySQL-server-5.6.26-1.linux_glibc2.5.x86_64.rpm</p><p>依赖报错：</p><p>缺 libaio,安装即可</p><p>yum -y install libaio </p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591167838484-7b1feb22-d0f2-4e70-92d6-83ba03ed250f.png" alt="image.png"></p><p>rpm -ivh MySQL-client-5.6.26-1.linux_glibc2.5.x86_64.rpm </p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591167825013-500f0280-dabc-4b53-aca7-1b2ce0b7b405.png" alt="image.png"></p><h4 id="7-3启动mysql"><a href="#7-3启动mysql" class="headerlink" title="7.3启动mysql"></a>7.3启动mysql</h4><p>service mysql start</p><p>高版本的mysql自带有root的密码：</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591167813568-035ad6af-43c4-4b0e-a5fe-729f79d98665.png" alt="image.png"></p><p>查看mysql密码：</p><p>cat /root/.mysql_secret</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591167804389-8f2103d1-ddbd-4d0c-b92d-f1e0da4e4411.png" alt="image.png"></p><p>登陆：</p><p>mysql -uroot -p密码</p><p> <img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591167793913-4abb51b7-c1d8-483b-9e0f-5a794970c092.png" alt="image.png"></p><p>修改密码：</p><p>SET PASSWORD = PASSWORD(‘自己的root密码’);</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591167782752-f4605b6a-9ab5-4a8b-b5e5-fe7bc15fc158.png" alt="image.png"></p><p>设置远程登陆权限：</p><p>给root用户授予从任何机器上登陆mysql服务器的权限：</p><p>mysql&gt; <strong>grant all privileges on *.* to ‘root’@’%’ identified by ‘root’ with grant option;</strong></p><p><strong>Query OK, 0 rows affected (0.00 sec)</strong></p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591167775204-623e0e2d-dcc7-4256-9548-947baf74b4a9.png" alt="image.png"></p><h1 id="第6章-自定义InputFormat-OutputFormat"><a href="#第6章-自定义InputFormat-OutputFormat" class="headerlink" title="第6章 自定义InputFormat,OutputFormat"></a>第6章 自定义InputFormat,OutputFormat</h1><p>在企业开发中，Hadoop框架自带的InputFormat类型不能满足所有应用场景，需要自定义InputFormat来解决实际问题。</p><h4 id="5-1需求"><a href="#5-1需求" class="headerlink" title="5.1需求"></a>5.1需求</h4><p>无论HDFS还是MapReduce，在处理小文件时效率都非常低，但又难免面临处理大量小文件的场景，此时，就需要有相应解决方案。可以自定义InputFormat实现小文件的合并。</p><p>将多个小文件合并成一个文件，并进行数据统计，存储到MySQL中。</p><h4 id="5-2需求分析"><a href="#5-2需求分析" class="headerlink" title="5.2需求分析"></a>5.2需求分析</h4><p><strong>自定义InputFormat****步骤如下：</strong></p><p>(1)   自定义一个类继承FileIputFormat</p><p>A.    重写isSplitable()方法，返回false不可切割</p><p>B.    重写createRecorReader()，创建自定义的RecordReader对象，并初始化</p><p>(2)   改写RecordReader，实现一次读取一个完整文件封装KV</p><p>A.    采用IO流一次读取一个文件输出到value中，因为设置了不可切片，最终把所有文件都封装到了value中</p><p>B.    获取文件路径信息+名称，并设置key</p><p><strong>在输出时使用OutputFormat****输出合并文件</strong></p><p>(1)  自定义一个类实现Writable，DBWritable</p><p>.</p><h4 id="5-3数据样例"><a href="#5-3数据样例" class="headerlink" title="5.3数据样例"></a>5.3数据样例</h4><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1489802/1591155232394-22b0587f-450d-4d3e-a2d4-32ec07175e7c.png" alt="image.png"></p><p> **<br>**</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据(HDFS) </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
